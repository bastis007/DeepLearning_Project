{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment : LLM: mBART\n",
        "\n",
        "**Author:** Gloria Isedu\n",
        "\n",
        "**Description:** Experiments to fine-tune and use a pre-trained mBART.\n",
        "\n",
        "**References:** https://huggingface.co/docs/transformers/model_doc/marian\n",
        "\n"
      ],
      "metadata": {
        "id": "0ZIcw6sWsqnZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install sentencepiece sacremoses Cython"
      ],
      "metadata": {
        "id": "Y7CAg-4Ps_km",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "946a9b10-5df3-4084-ebae-e7919c8e0e49"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.10/dist-packages (3.0.8)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from sacremoses) (2023.6.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from sacremoses) (4.66.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import MBartTokenizer, MBart50Tokenizer, TFMBartForConditionalGeneration, MBartForConditionalGeneration, TFBartForConditionalGeneration, AdamWeightDecay"
      ],
      "metadata": {
        "id": "jqmlLUGLNRoR"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from transformers import MBartTokenizer, TFBartForConditionalGeneration\n",
        "from transformers import MBartTokenizer, MBart50Tokenizer, TFMBartForConditionalGeneration, MBartForConditionalGeneration, TFBartForConditionalGeneration, AdamWeightDecay\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# def predict_and_compare(index, testX, testY, model, tokenizer, max_output_length=5):\n",
        "#     \"\"\" Predicts translation for a given index in the test set and compares with the ground truth. \"\"\"\n",
        "#     input_seq = testX[index:index+1]\n",
        "\n",
        "#     # Determine the total max_length (input length + desired output length)\n",
        "#     total_max_length = len(input_seq[0]) + max_output_length\n",
        "#     prediction = model.generate(input_seq, max_length=total_max_length, no_repeat_ngram_size=2)\n",
        "\n",
        "#     # Decode the prediction and input\n",
        "#     input_text = tokenizer.decode(input_seq[0], skip_special_tokens=True)\n",
        "#     predicted_text = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "\n",
        "#     # For ground truth\n",
        "#     ground_truth_text = tokenizer.decode(testY[index], skip_special_tokens=True)\n",
        "\n",
        "#     # Return results\n",
        "#     return input_text, predicted_text, ground_truth_text\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "#     gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "#     if gpus:\n",
        "#         try:\n",
        "#             # Set TensorFlow to use only one GPU\n",
        "#             tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "#             # Enable memory growth\n",
        "#             tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "#             print(\"Using GPU:\", gpus[0])\n",
        "#         except RuntimeError as e:\n",
        "#             # Memory growth must be set at program startup\n",
        "#             print(\"RuntimeError:\", e)\n",
        "#     else:\n",
        "#         raise SystemError(\"GPU device not found\")\n",
        "\n",
        "#     # --- 2. We define the global variable ---\n",
        "\n",
        "\n",
        "    BATCH_SIZE = 2#16\n",
        "    EPOCHS = 3#10\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "\n",
        "    # --- 3. We open the data and apply tokenization, with data generator ---\n",
        "\n",
        "    df = pd.read_csv('preprocessed_data.csv')\n",
        "    df = df[:121]\n",
        "    source_texts = df['en_tokens'].to_list()\n",
        "    target_texts = df['fr_tokens'].to_list()\n",
        "\n",
        "    source_train, source_val, target_train, target_val = train_test_split(source_texts, target_texts, test_size=VALIDATION_SPLIT, random_state=42)\n",
        "    # print(source_train)\n",
        "    # # We extract the test set first\n",
        "    # train_df, test_df = train_test_split(df, test_size=VALIDATION_SPLIT)\n",
        "\n",
        "    # Tokenize and pad  sequences using tokenizer\n",
        "    tokenizer = MBart50Tokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\", max_length=512)\n",
        "    tokenized_trainX = tokenizer(source_train, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
        "    tokenized_trainY = tokenizer(target_train, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
        "    tokenized_testX = tokenizer(source_val, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
        "    tokenized_testY = tokenizer(target_val, return_tensors=\"tf\", padding=True, truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "    # Create TensorFlow datasets\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((dict(tokenized_trainX), tokenized_trainY['input_ids']))\n",
        "    val_dataset = tf.data.Dataset.from_tensor_slices((dict(tokenized_testX), tokenized_testY['input_ids']))\n",
        "\n",
        "    # Batch and shuffle the datasets\n",
        "    train_dataset = train_dataset.batch(BATCH_SIZE).shuffle(buffer_size=len(source_train))\n",
        "    val_dataset = val_dataset.batch(BATCH_SIZE)\n",
        "\n",
        "    # Load pretrained mBART model\n",
        "    model = TFMBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "    # Adjust the model for conditional generation (translation)\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "    # Set up optimizer\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    # model.compile(optimizer=optimizer, loss=\"sparse_categorical_crossentropy\")\n",
        "    model.compile(optimizer=optimizer, loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True))\n",
        "\n",
        "    # Fine-tune the model with validation\n",
        "    model.trainable = True\n",
        "    epochs = 3\n",
        "    # Assuming tokenized_trainX and tokenized_trainY are BatchEncoding objects\n",
        "    input_ids_trainX = tokenized_trainX[\"input_ids\"].numpy()\n",
        "    attention_mask_trainX = tokenized_trainX[\"attention_mask\"].numpy()\n",
        "    input_ids_trainY = tokenized_trainY[\"input_ids\"].numpy()\n",
        "\n",
        "    # Train the model\n",
        "    model.fit(\n",
        "        x={\"input_ids\": input_ids_trainX, \"attention_mask\": attention_mask_trainX},\n",
        "        y=input_ids_trainY,\n",
        "        epochs=3,\n",
        "        batch_size=1\n",
        "    )\n",
        "    # model.fit(\n",
        "    # x=tokenized_trainX,\n",
        "    # y=tokenized_trainY[\"input_ids\"],\n",
        "    # epochs=3,\n",
        "    # batch_size=2,\n",
        "    # validation_split=0.2,\n",
        "    # # optimizer=optimizer,\n",
        "    # )\n",
        "\n",
        "    # # Iterate over the datasets\n",
        "    # for epoch in range(EPOCHS):\n",
        "    #     # Training loop\n",
        "    #     for batch in train_dataset:\n",
        "    #         with tf.GradientTape() as tape:\n",
        "    #             outputs = model(**batch[0], labels=batch[1])\n",
        "    #             loss = outputs.loss\n",
        "    #         gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    #         optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "\n",
        "    #     # Validation loop\n",
        "    #     for batch in val_dataset:\n",
        "    #         val_outputs = model(**batch[0], labels=batch[1])\n",
        "    #         val_loss = val_outputs.loss\n",
        "\n",
        "    #     # Print accuracy\n",
        "    #     print(f\"Epoch {epoch + 1}, Training Loss: {loss.numpy()}, Validation Loss: {val_loss.numpy()}\")\n",
        "\n",
        "\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "PBTJL33OtIg4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "2193ed1d-6ac7-478d-a013-f3943d792e92"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFMBartForConditionalGeneration.\n",
            "\n",
            "All the weights of TFMBartForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMBartForConditionalGeneration for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/3\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1674, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 278) and logits.shape=(1, 265, 250054)\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-67bc3d9a7c32>\u001b[0m in \u001b[0;36m<cell line: 27>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;31m# Train the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m     model.fit(\n\u001b[0m\u001b[1;32m    101\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minput_ids_trainX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mattention_mask_trainX\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m         \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids_trainY\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     68\u001b[0m             \u001b[0;31m# To get the full stack trace, call:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m             \u001b[0;31m# `tf.debugging.disable_traceback_filtering()`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1672\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1673\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1674\u001b[0;31m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompiled_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mregularization_losses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1675\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1676\u001b[0m         \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1401, in train_function  *\n        return step_function(self, iterator)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1384, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py\", line 1373, in run_step  **\n        outputs = model.train_step(data)\n    File \"/usr/local/lib/python3.10/dist-packages/transformers/modeling_tf_utils.py\", line 1674, in train_step\n        loss = self.compiled_loss(y, y_pred, sample_weight, regularization_losses=self.losses)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/engine/compile_utils.py\", line 277, in __call__\n        loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 143, in __call__\n        losses = call_fn(y_true, y_pred)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 270, in call  **\n        return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/losses.py\", line 2454, in sparse_categorical_crossentropy\n        return backend.sparse_categorical_crossentropy(\n    File \"/usr/local/lib/python3.10/dist-packages/keras/src/backend.py\", line 5775, in sparse_categorical_crossentropy\n        res = tf.nn.sparse_softmax_cross_entropy_with_logits(\n\n    ValueError: `labels.shape` must equal `logits.shape` except for the last dimension. Received: labels.shape=(1, 278) and logits.shape=(1, 265, 250054)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dJmbauzKroUs"
      },
      "outputs": [],
      "source": [
        "\n",
        "model = MBartForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Example translation\n",
        "text = \"Hello, how are you?\"\n",
        "input_ids = tokenizer.encode(text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "output_ids = model.generate(input_ids)\n",
        "translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input:\", text)\n",
        "print(\"Translation:\", translation)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "import tensorflow as tf\n",
        "from transformers import MBartTokenizer, TFBartForConditionalGeneration\n",
        "from transformers import AdamWeightDecay\n",
        "\n",
        "# Example text data\n",
        "source_texts = [\"This is an example sentence.\", \"Translate this sentence.\"]\n",
        "target_texts = [\"C'est une phrase exemple.\", \"Traduisez cette phrase.\"]\n",
        "\n",
        "# Tokenize and pad the sequences\n",
        "tokenizer = MBartTokenizer.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "tokenized_inputs = tokenizer(source_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "tokenized_outputs = tokenizer(target_texts, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "\n",
        "# Load pretrained mBART model for TensorFlow\n",
        "model = TFBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-many-mmt\")\n",
        "\n",
        "# Adjust the model for conditional generation (translation)\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Set up optimizer\n",
        "optimizer = AdamWeightDecay(model.parameters, learning_rate=5e-5)\n",
        "\n",
        "# Fine-tune the model\n",
        "model.train()\n",
        "for epoch in range(3):  # Adjust the number of epochs as needed\n",
        "    with tf.GradientTape() as tape:\n",
        "        outputs = model(**tokenized_inputs, labels=tokenized_outputs['input_ids'])\n",
        "        loss = outputs.loss\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {loss.numpy()}\")\n",
        "\n",
        "# Save the fine-tuned model\n",
        "model.save_pretrained(\"fine_tuned_mbart_translation_tf\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# T5\n",
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load pretrained T5 model and tokenizer\n",
        "model_name = \"t5-base\"\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Example translation\n",
        "text = \"Translate the following sentence: Hello, how are you?\"\n",
        "input_ids = tokenizer.encode(\"translate English to French: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
        "output_ids = model.generate(input_ids)\n",
        "translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
        "\n",
        "print(\"Input:\", text)\n",
        "print(\"Translation:\", translation)\n"
      ],
      "metadata": {
        "id": "g9DAgxxNt5Cq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}