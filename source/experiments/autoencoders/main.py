# import source
import tensorfow as tf
import pandas as pd
# import DeepLearning_Project.source.experiments.autoencoders.transformer_attention as ae
import pickle
import sys
sys.path.append('../../../../source')
from preprocessing import preprocessing_gloria as pre
from experiments.autoencoders import transformer_attention as ab_ae

# make constants
MODEL_NAME = 'distilbert-base-uncased'
MAX_LEN = 20

# preprocessing
original_translation_dataset = pre.load_data()

# just for set up
translation_dataset = original_translation_dataset[:101]

translation_dataset = pre.save_punct_free(df=translation_dataset)
translation_dataset = pre.lower_case(translation_dataset)
tok_translation_dataset = pre.tokenize_data(translation_dataset)
# Handling missing data
translation_dataset.dropna(subset=['en', 'fr'], inplace=True)
# sequencing
# english
en_word_idx, en_idx_word, en_sequences = pre.get_word_index(translation_dataset['en'])

# padding
en_padded_seqs, max_len_en = pre.add_padding(column=translation_dataset['en'], sequences=en_sequences)
X = en_padded_seqs

# french
# sequencing
fr_word_idx, fr_idx_word, fr_sequences = pre.get_word_index(translation_dataset['fr'])

# padding
fr_padded_seqs, max_len_fr = pre.add_padding(column=translation_dataset['fr'], sequences=fr_sequences)
y = fr_padded_seqs

# pickle.dump( X, open( "X.pkl", "wb" ) )
# pickle.dump( y, open( "y.pkl", "wb" ) )
# X = pickle.load( open( "X.pkl", "rb" ) )
# y = pickle.load( open( "y.pkl", "rb" ) )



# # Instantiate the encoder.
# sample_encoder = ae.Encoder(num_layers=4,
#                          d_model=512,
#                          num_heads=8,
#                          dff=2048,
#                          vocab_size=8500)

# sample_encoder_output = sample_encoder(en_padded_seqs, training=True)

# # Print the shape.
# print(pt.shape)
# print(sample_encoder_output.shape)  # Shape `(batch_size, input_seq_len, d_model)`


# sample_decoder_layer = ae.DecoderLayer(d_model=512, num_heads=8, dff=2048)

# sample_decoder_layer_output = sample_decoder_layer(
#     x=en_emb, context=pt_emb)

# print(en_emb.shape)
# print(pt_emb.shape)
# print(sample_decoder_layer_output.shape)  # `(batch_size, seq_len, d_model)`

















# # # ATTENTION BASED AUTOENCODER
# # # set constants
# # BATCH_SIZE = 64
# # BUFFER_SIZE = len(X)
# # UNITS = 256
# # steps_per_epoch = BUFFER_SIZE//BATCH_SIZE
# # embedding_dims = 256
# # rnn_units = 1024
# # dense_units = 1024
# # Tx = max_len_en
# # Ty = max_len_fr 

# # input_size = None
# # output_size = None

# # tf.keras.backend.clear_session() # Resets all state generated by Keras.
# # tf.random.set_seed(42)

# # train_ds = from_sentences_dataset(
# #     sentences_en_train, sentences_fr_train, shuffle=True, seed=42
# # )
# # easy_valid_ds = from_sentences_dataset(sentences_en_valid, sentences_fr_valid)

# # bidirect_encoder_decoder = ab_ae.BidirectionalEncoderDecoderWithAttention(max_sentence_len=15)
# # bidirect_history = adapt_compile_and_fit(
# #     bidirect_encoder_decoder,
# #     easy_train_ds,
# #     easy_valid_ds,
# #     init_lr=0.01,
# #     lr_decay_rate=0.01,
# #     colorama_verbose=True,
# # )

# # # # ENCODER
# # # encoderNN = ab_ae.EncoderNN(input_dim=input_size, output_dim=embedding_dims, dim_rnn_output=rnn_units)

# # # # DECODER
# # # decoderNN = ab_ae.DecoderNN(input_dim=output_size, output_dim=embedding_dims, 
# # #                             dim_rnn_output=rnn_units, attention_mech_depth=dense_units,
# # #                             batch_size=BATCH_SIZE, max_en_X=Tx)

# # # # OPTIMIZER
# # # optimizer = tf.keras.optimizers.Adam()
     
# # # loss_fn = ab_ae.loss_function()

