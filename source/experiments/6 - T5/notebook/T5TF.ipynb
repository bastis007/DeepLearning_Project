{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbPRMekGuGkz"
      },
      "source": [
        "# Experiment VI : Transformers: T5 Pre-trained Transformer\n",
        "\n",
        "**Author:** Felipe Cortes Jaramillo\n",
        "\n",
        "**Description:** Experiments to fine-tune and use a pre-trained T5 Transformer.\n",
        "\n",
        "**References:** https://huggingface.co/docs/transformers/model_doc/t5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgt3QPSADolX",
        "outputId": "4835f2ee-5732-4be4-9a1c-595fa3255432"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Let's load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmhX9mkDu2k",
        "outputId": "426bbf66-9fba-4c72-8e7b-ef71ec51c6ed"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Collecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses\n",
            "  Downloading sacremoses-0.1.1-py3-none-any.whl (897 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.5/897.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacrebleu\n",
            "  Downloading sacrebleu-2.4.0-py3-none-any.whl (106 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m106.3/106.3 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rouge-score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
            "Collecting bert-score\n",
            "  Downloading bert_score-0.3.13-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.1/61.1 kB\u001b[0m \u001b[31m10.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyter3\n",
            "  Downloading pyter3-0.3-py3-none-any.whl (4.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Collecting portalocker (from sacrebleu)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Collecting colorama (from sacrebleu)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
            "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n",
            "Building wheels for collected packages: rouge-score\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24933 sha256=f60746ddd6793ce6c507d4ed9f68819c65f35d87773fabac82f62855e79fbc98\n",
            "  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n",
            "Successfully built rouge-score\n",
            "Installing collected packages: sentencepiece, pyter3, sacremoses, rapidfuzz, portalocker, colorama, sacrebleu, rouge-score, jiwer, bert-score\n",
            "Successfully installed bert-score-0.3.13 colorama-0.4.6 jiwer-3.0.3 portalocker-2.8.2 pyter3-0.3 rapidfuzz-3.6.1 rouge-score-0.1.2 sacrebleu-2.4.0 sacremoses-0.1.1 sentencepiece-0.1.99\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece sacremoses sacrebleu rouge-score jiwer bert-score pyter3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAtNZWz_DwC-"
      },
      "outputs": [],
      "source": [
        "# --- 1. We import the libraries we need ---\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import TFT5ForConditionalGeneration, T5Tokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "from sacrebleu import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from jiwer import wer\n",
        "from bert_score import score as bert_score\n",
        "import pyter\n",
        "import re\n",
        "import ast\n",
        "import os\n",
        "\n",
        "def predict_and_compare(index, testX, testY, model, tokenizer, max_output_length=5):\n",
        "    \"\"\" Predicts translation for a given index in the test set and compares with the ground truth. \"\"\"\n",
        "    input_seq = testX[index:index+1]\n",
        "\n",
        "    # Determine the total max_length (input length + desired output length)\n",
        "    total_max_length = len(input_seq[0]) + max_output_length\n",
        "    prediction = model.generate(input_seq, max_length=total_max_length, no_repeat_ngram_size=2)\n",
        "\n",
        "    # Decode the prediction and input\n",
        "    input_text = tokenizer.decode(input_seq[0], skip_special_tokens=True)\n",
        "    predicted_text = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "\n",
        "    # For ground truth\n",
        "    ground_truth_text = tokenizer.decode(testY[index], skip_special_tokens=True)\n",
        "\n",
        "    # Return results\n",
        "    print(f'Prediction index for element finished: {index}')\n",
        "    return input_text, predicted_text, ground_truth_text\n",
        "\n",
        "def clean_predictions(predictions):\n",
        "    \"\"\" Cleans predictions for comparison purposes. \"\"\"\n",
        "    inputs, preds, truths = zip(*predictions)\n",
        "\n",
        "    # Join the tokenized words into sentences\n",
        "    preds = [\" \".join(pred) if isinstance(pred, list) else pred for pred in preds]\n",
        "    truths = [\" \".join(truth) if isinstance(truth, list) else truth for truth in truths]\n",
        "\n",
        "    # Clean preds\n",
        "    cleaned_preds = []\n",
        "    for current_pred in preds:\n",
        "      tokens = current_pred.strip('[]').split(',')\n",
        "\n",
        "      # Cleaning each token by removing special characters and extra quotes\n",
        "      cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token.strip()) for token in tokens]\n",
        "      split_tokens = [word for token in cleaned_tokens for word in token.split()]\n",
        "      cleaned_preds.append(split_tokens)\n",
        "\n",
        "    return zip(inputs, cleaned_preds, truths)\n",
        "\n",
        "class TimedCSVLogger(CSVLogger):\n",
        "    def __init__(self, filename, separator=',', append=False):\n",
        "        super().__init__(filename, separator, append)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        end_time = time.time()\n",
        "        logs['epoch_duration'] = end_time - self.epoch_start_time\n",
        "        logs['total_time'] = end_time - self.start_time\n",
        "        super().on_epoch_end(epoch, logs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Set TensorFlow to use only one GPU\n",
        "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "            # Enable memory growth\n",
        "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "            print(\"Using GPU:\", gpus[0])\n",
        "        except RuntimeError as e:\n",
        "            # Memory growth must be set at program startup\n",
        "            print(\"RuntimeError:\", e)\n",
        "    else:\n",
        "        raise SystemError(\"GPU device not found\")\n",
        "\n",
        "    # --- 2. We define the global variable ---\n",
        "\n",
        "    BATCH_SIZE = 32\n",
        "    EPOCHS = 100\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "    identifier = 't5_large_transformer_mt'\n",
        "\n",
        "    # --- 3. We open the data and apply tokenization, with data generator ---\n",
        "\n",
        "    df = pd.read_csv('./drive/MyDrive/data/dl/preprocessed_data.csv')\n",
        "\n",
        "    tokenizer = T5Tokenizer.from_pretrained('t5-large') # You can choose the model size (small, base, large, etc.)\n",
        "\n",
        "    # We extract the test set first\n",
        "    train_df, test_df = train_test_split(df, test_size=VALIDATION_SPLIT)\n",
        "    testX = tokenizer(test_df['en_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "    testY = tokenizer(test_df['fr_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "\n",
        "    src_texts = [\"translate English to French: \" + text for text in train_df['en_tokens'].tolist()]\n",
        "    tgt_texts = train_df['fr_tokens'].tolist()\n",
        "\n",
        "    model_inputs = tokenizer(src_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(tgt_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Prepare decoder_input_ids\n",
        "    start_token_id = tokenizer.pad_token_id\n",
        "    decoder_input_ids = np.full_like(labels['input_ids'], start_token_id)\n",
        "    decoder_input_ids[:, 1:] = labels['input_ids'][:,:-1]\n",
        "\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "    def data_generator(model_inputs, batch_size):\n",
        "        total_size = len(model_inputs['input_ids'])\n",
        "        for i in range(0, total_size, batch_size):\n",
        "            batch_input_ids = model_inputs['input_ids'][i:i + batch_size]\n",
        "            batch_attention_mask = model_inputs['attention_mask'][i:i + batch_size]\n",
        "            batch_decoder_input_ids = model_inputs['decoder_input_ids'][i:i + batch_size]\n",
        "            batch_labels = labels['input_ids'][i:i + batch_size]\n",
        "            batch_decoder_input_ids = model_inputs['decoder_input_ids'][i:i + batch_size]\n",
        "\n",
        "        yield ({\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask, \"decoder_input_ids\": batch_decoder_input_ids}, batch_labels)\n",
        "\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_size = int((1 - VALIDATION_SPLIT) * len(model_inputs['input_ids']))\n",
        "    train_dataset = (model_inputs[:train_size], labels[:train_size])\n",
        "    validation_dataset = (model_inputs[train_size:], labels[train_size:])\n",
        "\n",
        "    # Convert dataset and charg into model\n",
        "    train_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32, 'decoder_input_ids': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None]), 'decoder_input_ids': tf.TensorShape([None, None])},\n",
        "                   tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32, 'decoder_input_ids': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None]), 'decoder_input_ids': tf.TensorShape([None, None])},\n",
        "                   tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # --- 4.1 Define Callbacks ---\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
        "    csv_logger = TimedCSVLogger(f'./drive/MyDrive/data/dl/results/training_log/training_log_{identifier}.csv', append=True)\n",
        "\n",
        "    # --- 4.2 We define and compile the model ---\n",
        "    model = TFT5ForConditionalGeneration.from_pretrained('t5-large')\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    model_weights_path = f'./drive/MyDrive/data/dl/results/weights/weights_{identifier}.best'\n",
        "\n",
        "    if os.path.exists(model_weights_path):\n",
        "      model = tf.keras.models.load_model(model_weights_path)\n",
        "      print(\"Model weights loaded successfully!\")\n",
        "\n",
        "    # --- 5. We train the model ---\n",
        "    model.fit(train_data, validation_data=validation_data, epochs=EPOCHS, callbacks=[early_stopping, csv_logger])\n",
        "    model.save_weights(model_weights_path)\n",
        "\n",
        "    # ---6. Measure the performance ---\n",
        "    all_predictions_raw = []\n",
        "    for j in range(100):\n",
        "        input_text, predicted_text_raw, ground_truth_text = predict_and_compare(j, testX, testY, model, tokenizer)\n",
        "        all_predictions_raw.append((input_text, predicted_text_raw, ground_truth_text))\n",
        "    all_predictions = clean_predictions(all_predictions_raw)\n",
        "\n",
        "    with open(f'./drive/MyDrive/data/dl/results/predictions/model_predictions_{identifier}.txt', 'w', encoding='utf-8') as file:\n",
        "      for input_text, predicted_text, ground_truth in all_predictions:\n",
        "          # Format the input_text list\n",
        "          input_text = ast.literal_eval(input_text)\n",
        "          ground_truth = ast.literal_eval(ground_truth)\n",
        "\n",
        "          formatted_input_text = \"Input (English): \" + \" \".join(f\"'{word}'\" for word in input_text)\n",
        "          formatted_pred_text = \"Predicted (French): \" + \" \".join(f\"'{word}'\" for word in predicted_text)\n",
        "          formatted_truth_text = \"Ground Truth (French): \" + \" \".join(f\"'{word}'\" for word in ground_truth)\n",
        "\n",
        "          file.write(formatted_input_text + \"\\n\")\n",
        "          file.write(formatted_pred_text + \"\\n\")\n",
        "          file.write(formatted_truth_text + \"\\n\")\n",
        "          file.write(\"----------\\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aiBnp6HUUFH3"
      },
      "outputs": [],
      "source": [
        "# End of notebook!"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
