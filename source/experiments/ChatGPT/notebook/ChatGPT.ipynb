{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Experiment IV : Pre-trained models: ChatGPT\n",
        "\n",
        "**Author:** Felipe Cortes Jaramillo\n",
        "\n",
        "**Description:** Experiments to fine-tune and use a pre-trained ChatGPT model\n",
        "\n",
        "**References:** https://huggingface.co/gpt2"
      ],
      "metadata": {
        "id": "WbPRMekGuGkz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgt3QPSADolX",
        "outputId": "2101dade-05c7-479e-e17a-8c9966e73b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Let's load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers sentencepiece sacremoses sacrebleu rouge-score jiwer bert-score pyter3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmhX9mkDu2k",
        "outputId": "eba005f6-6d37-4f2d-8c28-15bcd25944f3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: sacrebleu in /usr/local/lib/python3.10/dist-packages (2.4.0)\n",
            "Requirement already satisfied: rouge-score in /usr/local/lib/python3.10/dist-packages (0.1.2)\n",
            "Requirement already satisfied: jiwer in /usr/local/lib/python3.10/dist-packages (3.0.3)\n",
            "Requirement already satisfied: bert-score in /usr/local/lib/python3.10/dist-packages (0.3.13)\n",
            "Requirement already satisfied: pyter3 in /usr/local/lib/python3.10/dist-packages (0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (2.8.2)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.9.0)\n",
            "Requirement already satisfied: colorama in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (0.4.6)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu) (4.9.4)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from rouge-score) (3.8.1)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from rouge-score) (1.16.0)\n",
            "Requirement already satisfied: rapidfuzz<4,>=3 in /usr/local/lib/python3.10/dist-packages (from jiwer) (3.6.1)\n",
            "Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from bert-score) (2.1.0+cu121)\n",
            "Requirement already satisfied: pandas>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from bert-score) (1.5.3)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from bert-score) (3.7.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.0.1->bert-score) (2023.3.post1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.0.0->bert-score) (2.1.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (4.47.2)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (1.4.5)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->bert-score) (3.1.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.0.0->bert-score) (2.1.3)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.0.0->bert-score) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 1. We import the libraries we need ---\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "import re\n",
        "import ast\n",
        "import os\n",
        "\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, CSVLogger, EarlyStopping\n",
        "\n",
        "from sacrebleu import corpus_bleu\n",
        "from rouge_score import rouge_scorer\n",
        "from jiwer import wer\n",
        "from bert_score import score as bert_score\n",
        "import pyter\n",
        "\n",
        "def predict_and_compare(index, testX, testY, model, tokenizer, max_output_length=5):\n",
        "    \"\"\" Predicts translation for a given index in the test set and compares with the ground truth. \"\"\"\n",
        "    input_seq = testX[index:index+1]\n",
        "\n",
        "    # Determine the total max_length (input length + desired output length)\n",
        "    total_max_length = len(input_seq[0]) + max_output_length\n",
        "    prediction = model.generate(input_seq, max_length=total_max_length, no_repeat_ngram_size=2)\n",
        "\n",
        "    # Decode the prediction and input\n",
        "    input_text = tokenizer.decode(input_seq[0], skip_special_tokens=True)\n",
        "    predicted_text = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "\n",
        "    # For ground truth\n",
        "    ground_truth_text = tokenizer.decode(testY[index], skip_special_tokens=True)\n",
        "\n",
        "    # Return results\n",
        "    print(f'Prediction index for element finished: {index}')\n",
        "    return input_text, predicted_text, ground_truth_text\n",
        "\n",
        "def clean_predictions(predictions):\n",
        "    \"\"\" Cleans predictions for comparison purposes. \"\"\"\n",
        "    inputs, preds, truths = zip(*predictions)\n",
        "\n",
        "    # Join the tokenized words into sentences\n",
        "    preds = [\" \".join(pred) if isinstance(pred, list) else pred for pred in preds]\n",
        "    truths = [\" \".join(truth) if isinstance(truth, list) else truth for truth in truths]\n",
        "\n",
        "    # Clean preds\n",
        "    cleaned_preds = []\n",
        "    for current_pred in preds:\n",
        "      tokens = current_pred.strip('[]').split(',')\n",
        "\n",
        "      # Cleaning each token by removing special characters and extra quotes\n",
        "      cleaned_tokens = [re.sub(r'[^\\w\\s]', '', token.strip()) for token in tokens]\n",
        "      split_tokens = [word for token in cleaned_tokens for word in token.split()]\n",
        "      cleaned_preds.append(split_tokens)\n",
        "\n",
        "    return zip(inputs, cleaned_preds, truths)\n",
        "\n",
        "class TimedCSVLogger(CSVLogger):\n",
        "    def __init__(self, filename, separator=',', append=False):\n",
        "        super().__init__(filename, separator, append)\n",
        "        self.start_time = time.time()\n",
        "\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        self.epoch_start_time = time.time()\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        end_time = time.time()\n",
        "        logs['epoch_duration'] = end_time - self.epoch_start_time\n",
        "        logs['total_time'] = end_time - self.start_time\n",
        "        super().on_epoch_end(epoch, logs)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Set TensorFlow to use only one GPU\n",
        "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "            # Enable memory growth\n",
        "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "            print(\"Using GPU:\", gpus[0])\n",
        "        except RuntimeError as e:\n",
        "            # Memory growth must be set at program startup\n",
        "            print(\"RuntimeError:\", e)\n",
        "    else:\n",
        "        raise SystemError(\"GPU device not found\")\n",
        "\n",
        "    # --- 2. We define the global variable ---\n",
        "\n",
        "    BATCH_SIZE = 12\n",
        "    EPOCHS = 1\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "    identifier = 'chatgpt_mt'\n",
        "\n",
        "    # --- 3. We open the data and apply tokenization, with data generator ---\n",
        "\n",
        "    df = pd.read_csv('./drive/MyDrive/data/dl/preprocessed_data.csv')\n",
        "    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "\n",
        "    # Set the EOS token as the padding token\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # We extract the test set first\n",
        "    train_df, test_df = train_test_split(df, test_size=VALIDATION_SPLIT)\n",
        "    testX = tokenizer(test_df['en_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "    testY = tokenizer(test_df['fr_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "\n",
        "    src_texts = train_df['en_tokens'].tolist()\n",
        "    tgt_texts = train_df['fr_tokens'].tolist()\n",
        "\n",
        "    model_inputs = tokenizer(src_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    # Concatenate source and target texts with a special token\n",
        "    train_texts = [\"{} <|endoftext|> {}\".format(src, tgt) for src, tgt in zip(src_texts, tgt_texts)]\n",
        "\n",
        "    # Tokenize\n",
        "    model_inputs = tokenizer(train_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(tgt_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    def data_generator(model_inputs, batch_size):\n",
        "      total_size = len(model_inputs['input_ids'])\n",
        "      for i in range(0, total_size, batch_size):\n",
        "          batch_input_ids = model_inputs['input_ids'][i:i + batch_size]\n",
        "          batch_attention_mask = model_inputs['attention_mask'][i:i + batch_size]\n",
        "          yield ({\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask}, batch_input_ids)\n",
        "\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_size = int((1 - VALIDATION_SPLIT) * len(model_inputs['input_ids']))\n",
        "    train_dataset = (model_inputs[:train_size], labels[:train_size])\n",
        "    validation_dataset = (model_inputs[train_size:], labels[train_size:])\n",
        "\n",
        "    # Convert dataset and charg into model\n",
        "    train_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None])}, tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None])}, tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    # --- 4.1 Define Callbacks ---\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=3, mode='min', verbose=1)\n",
        "    csv_logger = TimedCSVLogger(f'./drive/MyDrive/data/dl/results/training_log/training_log_{identifier}.csv', append=True)\n",
        "\n",
        "    # --- 4.2 We define and compile the model ---\n",
        "    model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    model_weights_path = f'./drive/MyDrive/data/dl/results/weights/weights_{identifier}.best'\n",
        "\n",
        "    if os.path.exists(model_weights_path):\n",
        "      model = tf.keras.models.load_model(model_weights_path)\n",
        "      print(\"Model weights loaded successfully!\")\n",
        "\n",
        "    # --- 5. We train the model ---\n",
        "    model.fit(train_data, validation_data=validation_data, epochs=EPOCHS, callbacks=[early_stopping, csv_logger])\n",
        "    model.save_weights(model_weights_path)\n",
        "\n",
        "    # ---6. Measure the performance ---\n",
        "    all_predictions_raw = []\n",
        "    for j in range(100):\n",
        "        input_text, predicted_text_raw, ground_truth_text = predict_and_compare(j, testX, testY, model, tokenizer)\n",
        "        all_predictions_raw.append((input_text, predicted_text_raw, ground_truth_text))\n",
        "    all_predictions = clean_predictions(all_predictions_raw)\n",
        "\n",
        "    with open(f'./drive/MyDrive/data/dl/results/predictions/model_predictions_{identifier}.txt', 'w', encoding='utf-8') as file:\n",
        "      for input_text, predicted_text, ground_truth in all_predictions:\n",
        "          # Format the input_text list\n",
        "          input_text = ast.literal_eval(input_text)\n",
        "          ground_truth = ast.literal_eval(ground_truth)\n",
        "\n",
        "          formatted_input_text = \"Input (English): \" + \" \".join(f\"'{word}'\" for word in input_text)\n",
        "          formatted_pred_text = \"Predicted (French): \" + \" \".join(f\"'{word}'\" for word in predicted_text)\n",
        "          formatted_truth_text = \"Ground Truth (French): \" + \" \".join(f\"'{word}'\" for word in ground_truth)\n",
        "\n",
        "          file.write(formatted_input_text + \"\\n\")\n",
        "          file.write(formatted_pred_text + \"\\n\")\n",
        "          file.write(formatted_truth_text + \"\\n\")\n",
        "          file.write(\"----------\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 653
        },
        "id": "lAtNZWz_DwC-",
        "outputId": "bd9cf596-b9f5-41be-9f52-f97cab970609"
      },
      "execution_count": 3,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "538/538 [==============================] - 1002s 2s/step - loss: nan - val_loss: nan - epoch_duration: 1001.5406 - total_time: 1007.2840\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "The current model class (TFGPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'TFGPT2LMHeadModel'}",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-eee6166af781>\u001b[0m in \u001b[0;36m<cell line: 74>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    176\u001b[0m     \u001b[0mall_predictions_raw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m         \u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_text_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict_and_compare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtestY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m         \u001b[0mall_predictions_raw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted_text_raw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mground_truth_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mall_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_predictions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_predictions_raw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-3-eee6166af781>\u001b[0m in \u001b[0;36mpredict_and_compare\u001b[0;34m(index, testX, testY, model, tokenizer, max_output_length)\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Determine the total max_length (input length + desired output length)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mtotal_max_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmax_output_length\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m     \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_seq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtotal_max_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mno_repeat_ngram_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;31m# Decode the prediction and input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, seed, **kwargs)\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;31m# 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 715\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_model_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m         \u001b[0;31m# priority: `generation_config` argument > `model.generation_config` (the default generation config)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/generation/tf_utils.py\u001b[0m in \u001b[0;36m_validate_model_class\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    618\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mgenerate_compatible_classes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m                 \u001b[0mexception_message\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Please use one of the following classes instead: {generate_compatible_classes}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexception_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_validate_model_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_kwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: The current model class (TFGPT2Model) is not compatible with `.generate()`, as it doesn't have a language model head. Please use one of the following classes instead: {'TFGPT2LMHeadModel'}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# End of notebook!"
      ],
      "metadata": {
        "id": "aiBnp6HUUFH3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}