{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WbPRMekGuGkz"
      },
      "source": [
        "# Experiment IV : Transformers: Marian Pre-trained Transformer\n",
        "\n",
        "**Author:** Felipe Cortes Jaramillo\n",
        "\n",
        "**Description:** Experiments to fine-tune and use a pre-trained Marian Transformer.\n",
        "\n",
        "**References:** https://huggingface.co/docs/transformers/model_doc/marian"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vgt3QPSADolX",
        "outputId": "6ed67887-d030-48a1-d03a-097c1ab1f64b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Let's load the data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMmhX9mkDu2k",
        "outputId": "663e9d37-bf07-4e54-ed05-e49339509c02"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.35.2)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.10/dist-packages (0.1.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.0)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from sacremoses) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses) (1.3.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.11.17)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers sentencepiece sacremoses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lAtNZWz_DwC-",
        "outputId": "2854c794-c892-4701-a865-78f1040fd0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using GPU: PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/tokenization_utils_base.py:3856: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n",
            "  warnings.warn(\n",
            "All model checkpoint layers were used when initializing TFMarianMTModel.\n",
            "\n",
            "All the layers of TFMarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-en-fr.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFMarianMTModel for predictions without further training.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 69s 69s/step - loss: 4.5837 - val_loss: 0.7071\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.7540 - val_loss: 0.3799\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4218 - val_loss: 0.3896\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4360 - val_loss: 0.3883\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4378 - val_loss: 0.3695\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4339 - val_loss: 0.3493\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.4058 - val_loss: 0.3341\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3788 - val_loss: 0.3213\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3608 - val_loss: 0.3088\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 3s 3s/step - loss: 0.3509 - val_loss: 0.2964\n"
          ]
        }
      ],
      "source": [
        "# --- 1. We import the libraries we need ---\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "from transformers import TFMarianMTModel, MarianTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "def predict_and_compare(index, testX, testY, model, tokenizer, max_output_length=5):\n",
        "    \"\"\" Predicts translation for a given index in the test set and compares with the ground truth. \"\"\"\n",
        "    input_seq = testX[index:index+1]\n",
        "\n",
        "    # Determine the total max_length (input length + desired output length)\n",
        "    total_max_length = len(input_seq[0]) + max_output_length\n",
        "    prediction = model.generate(input_seq, max_length=total_max_length, no_repeat_ngram_size=2)\n",
        "\n",
        "    # Decode the prediction and input\n",
        "    input_text = tokenizer.decode(input_seq[0], skip_special_tokens=True)\n",
        "    predicted_text = tokenizer.decode(prediction[0], skip_special_tokens=True)\n",
        "\n",
        "    # For ground truth\n",
        "    ground_truth_text = tokenizer.decode(testY[index], skip_special_tokens=True)\n",
        "\n",
        "    # Return results\n",
        "    return input_text, predicted_text, ground_truth_text\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "\n",
        "    gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "    if gpus:\n",
        "        try:\n",
        "            # Set TensorFlow to use only one GPU\n",
        "            tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "\n",
        "            # Enable memory growth\n",
        "            tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "\n",
        "            print(\"Using GPU:\", gpus[0])\n",
        "        except RuntimeError as e:\n",
        "            # Memory growth must be set at program startup\n",
        "            print(\"RuntimeError:\", e)\n",
        "    else:\n",
        "        raise SystemError(\"GPU device not found\")\n",
        "\n",
        "    # --- 2. We define the global variable ---\n",
        "\n",
        "    BATCH_SIZE = 16\n",
        "    EPOCHS = 10\n",
        "    VALIDATION_SPLIT = 0.2\n",
        "\n",
        "    # --- 3. We open the data and apply tokenization, with data generator ---\n",
        "\n",
        "    df = pd.read_csv('./drive/MyDrive/data/dl/preprocessed_data.csv')\n",
        "    tokenizer = MarianTokenizer.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
        "\n",
        "    # We extract the test set first\n",
        "    train_df, test_df = train_test_split(df, test_size=VALIDATION_SPLIT)\n",
        "    testX = tokenizer(test_df['en_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "    testY = tokenizer(test_df['fr_tokens'].tolist(), return_tensors='tf', padding=True, truncation=True, max_length=512)['input_ids']\n",
        "\n",
        "\n",
        "    src_texts = train_df['en_tokens'].tolist()\n",
        "    tgt_texts = train_df['fr_tokens'].tolist()\n",
        "\n",
        "    model_inputs = tokenizer(src_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(tgt_texts, return_tensors='tf', padding=True, truncation=True, max_length=512)\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "\n",
        "    # Prepare decoder_input_ids\n",
        "    start_token_id = tokenizer.pad_token_id\n",
        "    decoder_input_ids = np.full_like(labels['input_ids'], start_token_id)\n",
        "    decoder_input_ids[:, 1:] = labels['input_ids'][:,:-1]\n",
        "\n",
        "    model_inputs[\"decoder_input_ids\"] = decoder_input_ids\n",
        "\n",
        "    def data_generator(model_inputs, batch_size):\n",
        "        total_size = len(model_inputs['input_ids'])\n",
        "        for i in range(0, total_size, batch_size):\n",
        "            batch_input_ids = model_inputs['input_ids'][i:i + batch_size]\n",
        "            batch_attention_mask = model_inputs['attention_mask'][i:i + batch_size]\n",
        "            batch_decoder_input_ids = model_inputs['decoder_input_ids'][i:i + batch_size]\n",
        "            batch_labels = labels['input_ids'][i:i + batch_size]\n",
        "            batch_decoder_input_ids = model_inputs['decoder_input_ids'][i:i + batch_size]\n",
        "\n",
        "        yield ({\"input_ids\": batch_input_ids, \"attention_mask\": batch_attention_mask, \"decoder_input_ids\": batch_decoder_input_ids}, batch_labels)\n",
        "\n",
        "\n",
        "    # Split data into training and validation\n",
        "    train_size = int((1 - VALIDATION_SPLIT) * len(model_inputs['input_ids']))\n",
        "    train_dataset = (model_inputs[:train_size], labels[:train_size])\n",
        "    validation_dataset = (model_inputs[train_size:], labels[train_size:])\n",
        "\n",
        "    # Convert dataset and charg into model\n",
        "    train_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32, 'decoder_input_ids': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None]), 'decoder_input_ids': tf.TensorShape([None, None])},\n",
        "                   tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    validation_data = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(model_inputs, BATCH_SIZE),\n",
        "    output_types=({'input_ids': tf.int32, 'attention_mask': tf.int32, 'decoder_input_ids': tf.int32}, tf.int32),\n",
        "    output_shapes=({'input_ids': tf.TensorShape([None, None]), 'attention_mask': tf.TensorShape([None, None]), 'decoder_input_ids': tf.TensorShape([None, None])},\n",
        "                   tf.TensorShape([None, None]))\n",
        "    ).prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "\n",
        "    # --- 4. We define and compile the model ---\n",
        "    model = TFMarianMTModel.from_pretrained('Helsinki-NLP/opus-mt-en-fr')\n",
        "\n",
        "    optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "    loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "    model.compile(optimizer=optimizer, loss=loss)\n",
        "\n",
        "    # --- 5. We train the model ---\n",
        "    model.fit(train_data, validation_data=validation_data, epochs=EPOCHS)\n",
        "\n",
        "    # ---6. Measure the performance ---\n",
        "    all_predictions = []\n",
        "    for j in range(5):\n",
        "        input_text, predicted_text, ground_truth_text = predict_and_compare(j, testX, testY, model, tokenizer)\n",
        "        all_predictions.append((input_text, predicted_text, ground_truth_text))\n",
        "\n",
        "    # Print or analyze all_predictions\n",
        "    for prediction in all_predictions:\n",
        "        print(\"Input:\", prediction[0])\n",
        "        print(\"Predicted:\", prediction[1])\n",
        "        print(\"Ground Truth:\", prediction[2])\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
