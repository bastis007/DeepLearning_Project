import preprocessing as pre
import attention_based_ae as ab_ae
import pandas as pd
import tensorfow as tf
import tensorflow_addons as tfa


# preprocessing
original_translation_dataset = pre.load_data()

# just for set up
translation_dataset = original_translation_dataset[:101]

translation_dataset = pre.save_punct_free(df=translation_dataset)
translation_dataset = pre.lower_case(translation_dataset)
translation_dataset = pre.tokenize_data(translation_dataset)

# sequencing
# english
en_word_idx, en_idx_word, en_sequences = pre.get_word_index(translation_dataset['en'])

# padding
en_padded_seqs, max_len_en = pre.add_padding(column=translation_dataset['tokenized_en'], sequences=en_sequences)
X = en_padded_seqs

# french
# sequencing
fr_word_idx, fr_idx_word, fr_sequences = pre.get_word_index(translation_dataset['fr'])

# padding
fr_padded_seqs, max_len_fr = pre.add_padding(column=translation_dataset['tokenized_fr'], sequences=fr_sequences)
y = fr_padded_seqs

# ATTENTION BASED AUTOENCODER
# set constants
BATCH_SIZE = 64
BUFFER_SIZE = len(X)
UNITS = 256
steps_per_epoch = BUFFER_SIZE//BATCH_SIZE
embedding_dims = 256
rnn_units = 1024
dense_units = 1024
Tx = max_len_en
Ty = max_len_fr 

input_size = None
output_size = None

tf.keras.backend.clear_session() # Resets all state generated by Keras.
tf.random.set_seed(42)

train_ds = from_sentences_dataset(
    sentences_en_train, sentences_fr_train, shuffle=True, seed=42
)
easy_valid_ds = from_sentences_dataset(sentences_en_valid, sentences_fr_valid)

bidirect_encoder_decoder = ab_ae.BidirectionalEncoderDecoderWithAttention(max_sentence_len=15)
bidirect_history = adapt_compile_and_fit(
    bidirect_encoder_decoder,
    easy_train_ds,
    easy_valid_ds,
    init_lr=0.01,
    lr_decay_rate=0.01,
    colorama_verbose=True,
)

# # ENCODER
# encoderNN = ab_ae.EncoderNN(input_dim=input_size, output_dim=embedding_dims, dim_rnn_output=rnn_units)

# # DECODER
# decoderNN = ab_ae.DecoderNN(input_dim=output_size, output_dim=embedding_dims, 
#                             dim_rnn_output=rnn_units, attention_mech_depth=dense_units,
#                             batch_size=BATCH_SIZE, max_en_X=Tx)

# # OPTIMIZER
# optimizer = tf.keras.optimizers.Adam()
     
# loss_fn = ab_ae.loss_function()

